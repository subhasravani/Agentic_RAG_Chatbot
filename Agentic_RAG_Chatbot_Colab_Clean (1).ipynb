{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cc49aa9a",
      "metadata": {
        "id": "cc49aa9a"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu langchain PyMuPDF python-docx python-pptx pandas google-generativeai unstructured tiktoken openai streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bc761c8e",
      "metadata": {
        "id": "bc761c8e"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-google-genai langchain langchain-community google-generativeai -q\n",
        "\n",
        "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_google_genai.llms import GoogleGenerativeAI\n",
        "from langchain_core.documents import Document as LCDocument\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDPlJn5jQx0p8Svdv4KtkG2bHV0CJI-jXA\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "models = genai.list_models()\n",
        "for m in models:\n",
        "    # Only show models supporting generateContent\n",
        "    if \"generateContent\" in m.supported_generation_methods:\n",
        "        print(m.name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "9u320K5Wxy3l",
        "outputId": "55fd0a35-3288-4f03-95dc-195b5c720e4b"
      },
      "id": "9u320K5Wxy3l",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "llm = GoogleGenerativeAI(model=\"models/gemini-2.5-flash\")\n",
        "response = llm.invoke(\"Tell me about Paris?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRrL4C8pyxh4",
        "outputId": "3ed1f7bb-23a6-4f13-c270-ce3a3205ac82"
      },
      "id": "fRrL4C8pyxh4",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paris, often called the **\"City of Lights\" (La Ville Lumi√®re)** and the **\"City of Love,\"** is the capital and most populous city of France. It's globally renowned for its art, fashion, gastronomy, culture, and iconic landmarks. More than just a city, Paris is an experience ‚Äì a captivating blend of history, romance, and modern vibrancy.\n",
            "\n",
            "Here's a breakdown of what makes Paris so special:\n",
            "\n",
            "1.  **Iconic Landmarks:**\n",
            "    *   **Eiffel Tower:** The undisputed symbol of Paris, offering breathtaking panoramic views of the city.\n",
            "    *   **Louvre Museum:** Home to thousands of works of art, including the Mona Lisa, Venus de Milo, and countless masterpieces. It's the world's largest art museum.\n",
            "    *   **Notre Dame Cathedral:** A stunning Gothic masterpiece, currently undergoing restoration after the 2019 fire, but still an awe-inspiring sight.\n",
            "    *   **Arc de Triomphe & Champs-√âlys√©es:** The grandest avenue in Paris, leading up to the monumental Arc de Triomphe, which commemorates French victories.\n",
            "    *   **Sacr√©-C≈ìur Basilica (Montmartre):** Perched atop the highest point in the city, offering incredible views and a bohemian atmosphere in the surrounding Montmartre neighborhood.\n",
            "    *   **Mus√©e d'Orsay:** Housed in a former train station, it's famous for its vast collection of Impressionist and Post-Impressionist masterpieces.\n",
            "    *   **Seine River:** The heart of Paris, winding through the city and crossed by numerous beautiful bridges. Boat cruises on the Seine offer unique perspectives of the landmarks.\n",
            "\n",
            "2.  **Art & Culture:**\n",
            "    *   **Museums Galore:** Beyond the Louvre and Orsay, Paris boasts hundreds of museums, from the modern art of Centre Pompidou to the Rodin Museum, Picasso Museum, and many more niche collections.\n",
            "    *   **Bohemian History:** Neighborhoods like Montmartre and Saint-Germain-des-Pr√©s were once hubs for artists, writers, and intellectuals, and that artistic spirit still lingers.\n",
            "    *   **Opera & Theatre:** The Op√©ra Garnier is a magnificent architectural marvel and a world-renowned opera house. Paris also has a vibrant theatre scene.\n",
            "\n",
            "3.  **Gastronomy:**\n",
            "    *   **Culinary Capital:** Paris is a paradise for food lovers. From Michelin-starred restaurants to cozy bistros, traditional brasseries, and charming caf√©s, the culinary scene is diverse and exceptional.\n",
            "    *   **Pastries & Desserts:** Indulge in croissants, pain au chocolat, macarons, √©clairs, tarts, and countless other sweet treats from local patisseries.\n",
            "    *   **Cheese & Wine:** France's rich tradition of cheese and wine is on full display, with countless specialty shops and wine bars.\n",
            "    *   **Caf√© Culture:** Lingering at a sidewalk caf√©, sipping coffee, and people-watching is an essential Parisian experience.\n",
            "\n",
            "4.  **Fashion & Shopping:**\n",
            "    *   **Global Fashion Capital:** Paris is synonymous with haute couture and high fashion. You'll find flagship stores of the world's most luxurious brands on avenues like Avenue Montaigne and Rue du Faubourg Saint-Honor√©.\n",
            "    *   **Department Stores:** Grand department stores like Galeries Lafayette and Printemps offer stunning architecture, diverse shopping, and often rooftop views.\n",
            "    *   **Boutiques & Markets:** Explore charming independent boutiques, vintage shops, and bustling flea markets for unique finds.\n",
            "\n",
            "5.  **Ambiance & Lifestyle:**\n",
            "    *   **Romantic Charm:** The city's elegant Haussmannian architecture, tree-lined boulevards, charming squares, and the soft glow of streetlights create an undeniably romantic atmosphere.\n",
            "    *   **Walkable City:** Paris is best explored on foot, allowing you to discover hidden courtyards, quaint streets, and local life.\n",
            "    *   **Arrondissements:** The city is divided into 20 administrative districts (arrondissements), each with its own distinct character, from the trendy Marais to the chic 7th or the vibrant Latin Quarter.\n",
            "    *   **Parks & Gardens:** Beautiful green spaces like the Luxembourg Gardens, Tuileries Garden, and Parc des Buttes-Chaumont offer peaceful escapes from the urban bustle.\n",
            "\n",
            "**In essence, Paris is a city that constantly inspires, delights, and surprises.** It's a place where history comes alive, art is celebrated, and every corner seems to hold a new discovery. Whether you're seeking romance, culture, culinary adventures, or simply the joy of being in one of the world's most beautiful cities, Paris truly lives up to its legendary reputation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_from_pdf(file):\n",
        "    doc = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
        "    return \"\\n\".join(page.get_text() for page in doc)\n",
        "\n",
        "def load_text_from_csv(file):\n",
        "    df = pd.read_csv(file)\n",
        "    return df.to_string(index=False)\n",
        "\n",
        "def load_text_from_pptx(file):\n",
        "    prs = Presentation(file)\n",
        "    return \"\\n\".join(shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, \"text\"))\n",
        "\n",
        "\n",
        "\n",
        "from io import BytesIO\n",
        "from docx import Document\n",
        "\n",
        "def load_text_from_docx(file):\n",
        "    file.seek(0)\n",
        "    byte_stream = BytesIO(file.read())  # ‚úÖ Read the bytes\n",
        "    doc = Document(byte_stream)         # ‚úÖ Parse it using python-docx\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])  # ‚úÖ Extract text\n",
        "    return text                         # ‚úÖ Return a string (not BytesIO!)\n",
        "\n",
        "\n",
        "\n",
        "def load_text_from_txt(file):\n",
        "    return file.read().decode(\"utf-8\")\n",
        "\n",
        "def extract_text(file):\n",
        "    name = file.name.lower()\n",
        "    if name.endswith(\".pdf\"):\n",
        "        return load_text_from_pdf(file)\n",
        "    elif name.endswith(\".csv\"):\n",
        "        return load_text_from_csv(file)\n",
        "    elif name.endswith(\".pptx\"):\n",
        "        return load_text_from_pptx(file)\n",
        "    elif name.endswith(\".docx\"):\n",
        "        return load_text_from_docx(file)\n",
        "    elif name.endswith(\".txt\") or name.endswith(\".md\"):\n",
        "        return load_text_from_txt(file)\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "lIlXU1euzLT4"
      },
      "id": "lIlXU1euzLT4",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IngestionAgent:\n",
        "    def __init__(self, embedding_model):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    def handle(self, mcp_msg):\n",
        "        docs = []\n",
        "\n",
        "        for file in mcp_msg[\"payload\"][\"files\"]:\n",
        "            file.seek(0)\n",
        "            text = extract_text(file)\n",
        "            splits = self.text_splitter.split_text(text)\n",
        "            docs.extend([LCDocument(page_content=chunk) for chunk in splits])\n",
        "\n",
        "        self.vectorstore = FAISS.from_documents(docs, self.embedding_model)\n",
        "\n",
        "        return {\n",
        "            \"sender\": \"IngestionAgent\",\n",
        "            \"receiver\": \"RetrievalAgent\",\n",
        "            \"type\": \"DOCUMENT_INGESTED\",\n",
        "            \"trace_id\": mcp_msg[\"trace_id\"],\n",
        "            \"payload\": {\"doc_count\": len(docs)}\n",
        "        }\n",
        "\n",
        "\n",
        "class RetrievalAgent:\n",
        "    def __init__(self, vectorstore):\n",
        "        self.vectorstore = vectorstore\n",
        "\n",
        "    def handle(self, mcp_msg):\n",
        "        query = mcp_msg[\"payload\"][\"query\"]\n",
        "        docs = self.vectorstore.similarity_search(query, k=5)\n",
        "        context = [doc.page_content for doc in docs]\n",
        "        return {\n",
        "            \"sender\": \"RetrievalAgent\",\n",
        "            \"receiver\": \"LLMResponseAgent\",\n",
        "            \"type\": \"RETRIEVAL_RESULT\",\n",
        "            \"trace_id\": mcp_msg[\"trace_id\"],\n",
        "            \"payload\": {\n",
        "                \"retrieved_context\": context,\n",
        "                \"query\": query\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "class LLMResponseAgent:\n",
        "    def __init__(self, model=\"models/gemini-1.5-flash-latest\"):\n",
        "        self.llm = GoogleGenerativeAI(model=model)\n",
        "\n",
        "    def handle(self, mcp_msg):\n",
        "        context = \"\\n\".join(mcp_msg[\"payload\"][\"retrieved_context\"])\n",
        "        query = mcp_msg[\"payload\"][\"query\"]\n",
        "\n",
        "        prompt = f\"\"\"Use the following context to answer the user's question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "        return {\n",
        "            \"sender\": \"LLMResponseAgent\",\n",
        "            \"receiver\": \"UI\",\n",
        "            \"type\": \"ANSWER\",\n",
        "            \"trace_id\": mcp_msg[\"trace_id\"],\n",
        "            \"payload\": {\n",
        "                \"answer\": response,\n",
        "                \"sources\": context[:2]\n",
        "            }\n",
        "        }\n"
      ],
      "metadata": {
        "id": "JLFRe000zQJY"
      },
      "id": "JLFRe000zQJY",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ 1. Upload file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "uploaded_files = [open(name, \"rb\") for name in uploaded]\n",
        "\n",
        "# ‚úÖ 2. Build embedding model\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# ‚úÖ 3. Initialize ingestion agent\n",
        "ingestion_agent = IngestionAgent(embedding_model)\n",
        "\n",
        "# ‚úÖ 4. Create & send MCP message to ingestion agent\n",
        "import uuid\n",
        "trace_id = str(uuid.uuid4())\n",
        "ingest_msg = {\n",
        "    \"sender\": \"UI\",\n",
        "    \"receiver\": \"IngestionAgent\",\n",
        "    \"type\": \"UPLOAD\",\n",
        "    \"trace_id\": trace_id,\n",
        "    \"payload\": {\"files\": uploaded_files}\n",
        "}\n",
        "\n",
        "# ‚úÖ 5. Run ingestion (build FAISS in memory)\n",
        "ingest_response = ingestion_agent.handle(ingest_msg)\n",
        "\n",
        "# ‚úÖ ‚úÖ 6. Now use in-memory vectorstore for retrieval\n",
        "retrieval_agent = RetrievalAgent(ingestion_agent.vectorstore)\n",
        "\n",
        "# ‚úÖ 7. Setup LLM Agent\n",
        "llm_agent = LLMResponseAgent()\n",
        "\n",
        "# ‚úÖ 8. Ask question via MCP\n",
        "query = \"What are the key ideas in the uploaded document?\"\n",
        "retrieval_msg = {\n",
        "    \"sender\": \"UI\",\n",
        "    \"receiver\": \"RetrievalAgent\",\n",
        "    \"type\": \"QUESTION\",\n",
        "    \"trace_id\": trace_id,\n",
        "    \"payload\": {\"query\": query}\n",
        "}\n",
        "\n",
        "retrieval_response = retrieval_agent.handle(retrieval_msg)\n",
        "\n",
        "# ‚úÖ 9. Get final LLM response\n",
        "llm_response = llm_agent.handle(retrieval_response)\n",
        "print(\"üí¨ Answer:\\n\", llm_response[\"payload\"][\"answer\"])\n",
        "while True:\n",
        "    query = input(\"Ask a question (or type 'exit'): \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    trace_id = str(uuid.uuid4())\n",
        "    retrieval_msg = {\n",
        "        \"sender\": \"UI\",\n",
        "        \"receiver\": \"RetrievalAgent\",\n",
        "        \"type\": \"QUESTION\",\n",
        "        \"trace_id\": trace_id,\n",
        "        \"payload\": {\"query\": query}\n",
        "    }\n",
        "\n",
        "    retrieval_response = retrieval_agent.handle(retrieval_msg)\n",
        "    llm_response = llm_agent.handle(retrieval_response)\n",
        "\n",
        "    print(\"\\nüí¨ Answer:\", llm_response[\"payload\"][\"answer\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "IACM2X3r3RN-",
        "outputId": "ed37c4e2-01be-44bd-b1cc-a7a0fcbb6ade"
      },
      "id": "IACM2X3r3RN-",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a44d450b-6074-4aae-aa05-1a141fac7776\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a44d450b-6074-4aae-aa05-1a141fac7776\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 540, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 699, which is longer than the specified 500\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1668, which is longer than the specified 500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all in 1.docx to all in 1 (1).docx\n",
            "üí¨ Answer:\n",
            " The document showcases a portfolio of data science and software engineering projects.  Key ideas include:\n",
            "\n",
            "* **Expertise in various machine learning models:**  The author demonstrates proficiency in diverse models, including Random Forest, LightGBM, LSTM, and GAN-CNN-LSTM, applied to different tasks like GST compliance prediction, spam detection, and mental health condition forecasting.\n",
            "\n",
            "* **Natural Language Processing (NLP) capabilities:**  A significant project focuses on building a GPT-2 based text generation platform using LangChain and HuggingFace Transformers.\n",
            "\n",
            "* **Full-stack development skills:** The portfolio includes projects demonstrating front-end skills (React.js, JavaScript, Bootstrap) and back-end skills (Flask).\n",
            "\n",
            "* **Data visualization and analysis:**  Projects involve data exploration (EDA), feature engineering, and the creation of visualizations using libraries like Seaborn and Matplotlib.\n",
            "\n",
            "* **Deployment and accessibility:**  The author highlights the deployment of models through web interfaces (Flask for SpamShieldAI, and implied for the GPT-2 platform), making them accessible to non-technical users.\n",
            "\n",
            "Ask a question (or type 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Save FAISS vectorstore to local files\n",
        "ingestion_agent.vectorstore.save_local(\"faiss_store\")  # saves index + docstore\n"
      ],
      "metadata": {
        "id": "rcWwZliX37cL"
      },
      "id": "rcWwZliX37cL",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI\n",
        "\n",
        "# ‚úÖ Set Gemini API Key securely\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDPlJn5jQx0p8Svdv4KtkG2bHV0CJI-jXA\"\n",
        "\n",
        "# ‚úÖ Define embedding model\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# ‚úÖ Load FAISS vectorstore with deserialization allowed\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"faiss_store\", embeddings=embedding_model, allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "# ‚úÖ Load Gemini model (use supported model)\n",
        "llm = GoogleGenerativeAI(model=\"models/gemini-1.5-flash\")\n",
        "\n",
        "# ‚úÖ Streamlit UI\n",
        "st.title(\"üìÑ Gemini RAG Chatbot\")\n",
        "\n",
        "query = st.text_input(\"Ask a question about your document:\")\n",
        "\n",
        "if query:\n",
        "    docs = vectorstore.similarity_search(query, k=5)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    prompt = f\"\"\"Answer the question using only the context below.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    st.markdown(\"### üí¨ Answer:\")\n",
        "    st.write(response)  # ‚úÖ correct\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ1r_8h65aOr",
        "outputId": "36c17cc5-a592-436e-b688-577880ff0cf9"
      },
      "id": "JJ1r_8h65aOr",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaUVYz1x6TDH",
        "outputId": "70b6fd66-27d1-4575-b717-22557947964e"
      },
      "id": "uaUVYz1x6TDH",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.74.176.134:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0Kyour url is: https://twelve-states-juggle.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}