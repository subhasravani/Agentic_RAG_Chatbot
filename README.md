# Document Q&A Chatbot using Agentic RAG Architecture with MCP

This project presents a Document Question Answering Chatbot built with a modular, agent-based architecture using Retrieval-Augmented Generation (RAG). It supports multi-format document ingestion (PDF, DOCX, PPTX, TXT, CSV) and leverages the Model Context Protocol (MCP) for structured message passing between agents.

## ðŸ“Œ Project Overview

This chatbot enables users to ask questions based on uploaded documents and receive contextual answers generated by an LLM. The system is designed to be modular, explainable, and extendable through agent-based architecture and MCP.

---

## ðŸ§  Key Features

- **Multi-format document ingestion**: Supports PDF, DOCX, PPTX, TXT, and CSV.
- **Agent-based modular design**: Includes IngestionAgent, RetrievalAgent, and LLMResponseAgent.
- **FAISS vector store**: Efficient in-memory semantic search.
- **Google Gemini LLM**: Used for response generation.
- **Streamlit UI**: User-friendly web interface for upload and Q&A.
- **MCP integration**: Ensures clean communication flow using traceable messages.

---

## ðŸ—ï¸ Agent-Based Architecture

### ðŸ”¹ IngestionAgent
- Converts documents into text chunks
- Generates embeddings using Gemini Embedding API
- Stores them in FAISS vectorstore

### ðŸ”¹ RetrievalAgent
- Accepts user queries
- Performs similarity search on vectorstore
- Returns top-k relevant chunks

### ðŸ”¹ LLMResponseAgent
- Composes prompt using retrieved chunks
- Sends prompt to Gemini model
- Returns final response to user

---

## ðŸ” System Flow with MCP

1. **User uploads documents** via Streamlit
2. **IngestionAgent** processes and stores embeddings
3. **RetrievalAgent** handles user queries
4. **LLMResponseAgent** uses Gemini to generate the response
5. All communication happens through **MCP messages** containing:
   - Sender
   - Receiver
   - Type (UPLOAD / QUESTION)
   - Trace ID
   - Payload (files or query)

---

## ðŸ› ï¸ Tech Stack Used

- **Python**
- **LangChain**
- **Google Gemini API (Text + Embeddings)**
- **FAISS**
- **Streamlit** (for frontend)
- **Pydantic** (for message validation)

---

## ðŸŽ¯ Example Use Cases

- Summarizing uploaded research papers
- Answering specific questions from business reports
- Legal document analysis
- Academic note-based Q&A

---

## ðŸ“¸ UI Screenshots

> *Add screenshots of:*
> - File upload interface
> - Example question and LLM-generated answer

---

## ðŸš§ Challenges Faced

- Handling multiple file types and encoding issues
- Chunking large documents intelligently
- Managing errors in LLM model fallback/retries
- Serialization limitations with FAISS and Google Colab
- Google Gemini model version mismatch handling

---

## ðŸš€ Future Scope

- Add file upload from URLs or cloud storage
- Support chat history and conversational memory
- Integrate additional RAG models like Claude or Mistral
- Deploy using Docker and host on HuggingFace or Render

---



